{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12142019_Markov_chain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4aBSWch9tGm",
        "colab_type": "text"
      },
      "source": [
        "**Terminology**:\n",
        "- *Randomized algorithms*: Algorithms that make use of random number generators. \n",
        "\n",
        "- *Aperiodic*: \n",
        " - A state $s_i$ is aperiodic if the \"period\" of the state is 1. The period $d(s_i) := gcd\\{n\\ge1: (P^n)_{i,i}>0\\}$, which says after d (or any interger times of d) steps, it is possible (positive probability) to return to the same state $i$.  \n",
        "  - A Markov chain is aperiodic if all the states are aperiodic.\n",
        "\n",
        "- *Stationary distribution $\\pi$*: \n",
        "  - Def1. $\\pi P = \\pi$, where $P$ is the transition matrix of the Markov chain.\n",
        "  - Def2. The distribution $\\mu_n$ (at step n in a Markov chain) converges in total variance (t.v.) to $\\pi$, i.e., $\\lim_{n\\to\\infty}d_{TV}(\\mu_n, \\pi) = 0$\n",
        "- *Reversible distribution*: \n",
        "A probability distribution $\\pi$ is said to be reversible on the Markov chain (or the transition matrix) if for any $i,j\\in \\{1,2,...,k\\}$, $\\pi_i P_{i,j} = \\pi_j P_{j,i}$. A Markov chain is reversible if it has at least one reversible distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvkIFbEx9iix",
        "colab_type": "text"
      },
      "source": [
        "# 1. Stationary Distribution of a Markov Chain\n",
        "\n",
        "## Why do we need \"aperiodic\"ï¼Ÿ\n",
        "Stationary distribution is an important property in Markov chain theory. For a discrete time and finite state space Markov chain, being \"irreducible\" and \"aperiodic\" implies it has one and only one stationary distribution. \n",
        "\n",
        "Compared with irreducibility, aperiodicity is a less intuitive concept. \n",
        "\n",
        "For state $s_i$, the definition of aperiodicty tells us that $gcd(N_i) := gcd\\{n\\ge1: (P^n)_{i,i}>0\\}=1$, but it does not necessarily mean that 1 is an element in $N_i$.In other words, we don't have $P_{i,i}>0$ yet.\n",
        "\n",
        "However, being aperiodic leads us to Thm 4.1, which says \"there exists a $N<\\inf$ such that $(P^n)_{i,i}>0$ for all $n>N$\". In other words, when $n>N$ and the chain move forward one step, we can always go from state $i$ to $i$ with positive probability. This is very nice as it **gives hopes for convergence when $n$ goes to infinity**. The proof of Thm 4.1 utilizes a lemma from number theory which guarantees that all the $n$ after $N$ belongs to the set $N_i$. \n",
        "\n",
        "## Example: Aperiodic or periodic Markov chain?\n",
        "To determine whether a Markov chain is aperiodic or not, there are some creterions that can be applied. \n",
        "\n",
        "Step 1. We can draw a transition diagram from the transition matrix if the number of states are not too many. With the diagram, we can easily tell if the Markov chain is irreducible or not. \n",
        "\n",
        "Step 2. It can be shown that **if a Markov chain is irreducible, all of its states have the same period**. \n",
        "![Example 1. Transition diagrams of two Markov chains](https://miro.medium.com/max/2404/1*bDNsx76wQoE9uyWJby1zwQ@2x.png)\n",
        "Take a look at the two MCs in the figure above. From the transition diagram, both chains are irreducible (as the states can all be reached from each other). \n",
        "Therefore, we can look at only one of their states for the full information of periodicity. \n",
        "\n",
        "For the chain on the left, state 1 has periods of 2, 4, ..., therefore its period is 2. State 1 is periodic and then the whole chain is not aperiodic. Simimlarly, look at state 1 (in the center). It has periods of 3, 6, ..., and thus its period is 3. The MC on the right is not aperiodic either. \n",
        "\n",
        "A useful tip:**If a Markov chain is irreducible and it has a state $i$ such that $P_{i,i}>0$, then it is aperiodic.** (adapted from Prb 4.2. in the book)\n",
        "\n",
        "*Proof*: $P_{i,i} > 0$ means the chain can come to state $i$ from state i in one step, and thus the period of i (gcd{$n\\ge1: P^n_{i,i}>0$}) is 1. As we mentioned in step2, all the states have the same period for irreducible MC, and thus the chain is aperiodic. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDk1ntROh3D1",
        "colab_type": "text"
      },
      "source": [
        "# 2. Reversible and Stationary\n",
        "We have seen the definition of \"stationary\" distribution and \"reversible\" distribution in the terminology part, but it is not that obvious why a reversible distribution is also a stationary one for the Markov chain. Let's prove it here. \n",
        "\n",
        "A distribution $\\pi$ is stationary means the distribution remains the same after the transition. Mathematically, it is $\\pi P=\\pi$. We can also write it in element-wise format, $\\sum_i \\pi_{i} P_{i,j} = \\pi_j$. NOTE: The distribution on the right hand side of the equation would not be $\\pi$ (but a different distribution) if it is not stationary. \n",
        "\n",
        "If $\\pi$ is reversible, we have $$\\sum_i \\pi_i P_{i,j} =\\sum_j \\pi_j P_{j,i},$$ so $$\\sum_i \\pi_i P_{i,j} = \\sum_i \\pi_j P_{j,i} = \\pi_j \\sum_i P_{j,i} = \\pi_j$$ and therefore, $\\pi$ is also a stationary distribution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVlAPLqmhdoo",
        "colab_type": "text"
      },
      "source": [
        "# 3. MCMC\n",
        "This week, we will move one step forward, from theory to algorithms and see how a popular algorithm called Markov Chain Monte Carlo (MCMC) could help us solve various problems. \n",
        "\n",
        "## 3.1. How to construct a MCMC in general: a \"secret\" receipy. \n",
        "\n",
        "- Example: Gibbs sampler for \"hard-core\" model.\n",
        "\n",
        "- Example: Metropolis chain for ... (from my notes)\n",
        "\n",
        "## 3.2. What is MCMC good for? Sampling a distribution; approximate counting; etc. \n",
        "When we think of a Markov chain, a vivid image in our minds is the \"jump\" from one state to another. The jump is \"random\", following a probability distribution ($P_{i,j}$ for the jump from state $i$ to $j$), and simulating this random jump for an arbitrary distribution is not trivial especially when the state space is large. \n",
        "\n",
        "A \"naive\" algoirthm: \n",
        "\n",
        "Given a Markov chain with state space $S=\\{s_1, s_2, ..., s_k\\}$, we assume at step $n$, the state is $s_i$. Then the state at step $n+1$ is determined from $s_i$ and a generated random number $x (x\\in[0,1])$ in following way: \n",
        "\n",
        "$\\phi(s_i, x) = \\left\\{\n",
        "                \\begin{array}{ll}\n",
        "                  s_1, x\\in[0, P_{i,1}) \\\\\n",
        "                  s_2, x\\in[P_{i,1}, P_{i,1}+P_{i,2})\\\\\n",
        "                  ...\\\\\n",
        "                  s_k, x\\in[\\sum_{j=1}^{k-1} P_{i,j}, 1]\n",
        "                \\end{array}\n",
        "              \\right.\n",
        " $\n",
        "\n",
        "\n",
        "Note that in practice it is easy to generate a random number in uniform distribution (i.e., $x$), but if $P_{i,j}$ is arbitrary for different $j$, then we have to compare $x$ with all the possible range (k different ranges here) in the worst scenario. \n",
        "\n",
        "NOTE: Why is MCMC a better sampling method (in terms of computer simulation)? Code example: What we do when we don't have MCMC?\n",
        "\n",
        "NOTE: How can we estimate the distribution or even counts elements in a set without sampling all the possible states?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCK4N1kHkR1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example. Los Angeles' weather\n",
        "# If we assume there are only two types of weather: sunny and not-sunny, we can model\n",
        "# the weather at different time as a Markov chain (state s1 is 'sunny' and s2 \n",
        "# is 'not sunny').\n",
        "\n",
        "# In Los Angeles, as sunshine is much more common than other weathers, the\n",
        "# transition matrix is assumed to be the following\n",
        "# (from 'not-sunny' to 'sunny' is P12=0.5, from 'sunny' to 'not-sunny' is P21=0.1): \n",
        "P = [[0.5, 0.5], \n",
        "     [0.1, 0.9]]\n",
        "\n",
        "# An initial distribution: \n",
        "u0 = [0.5, 0.5]\n",
        "\n",
        "N = 10 # The number of steps to simulate\n",
        "\n",
        "from random import random\n",
        "\n",
        "def move_one_step(P, s_t, x):\n",
        "  \"\"\"\n",
        "  P: a list of list. \n",
        "    The transition matrix. \n",
        "  s_t: integer.\n",
        "    1 for 'not-sunny', 2 for 'sunny'.\n",
        "  x: float between 0 and 1.\n",
        "\n",
        "  # The code can be simplified, but we write out the \"raw\" idea here to\n",
        "  # make it easy to compare with the algorithm description. \n",
        "  \"\"\"\n",
        "  if s_t == 1:\n",
        "    # The if-statement below is the \"naive\" algorithm to select s_{t+1}:\n",
        "    if x < P[0][0]:\n",
        "      return 1\n",
        "    elif x < P[0][0] + P[0][1]:\n",
        "      return 2\n",
        "  \n",
        "  if s_t == 2:\n",
        "    if x < P[1][0]:\n",
        "      return 1\n",
        "    elif x < P[1][0] + P[1][1]:\n",
        "      return 2\n",
        "\n",
        "\n",
        "def simulate_N_steps(P, u0, N):\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}