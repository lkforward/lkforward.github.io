{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12142019_Markov_chain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4aBSWch9tGm",
        "colab_type": "text"
      },
      "source": [
        "**Terminology**:\n",
        "- *Randomized algorithms*: Algorithms that make use of random number generators. \n",
        "\n",
        "- *Aperiodic*: \n",
        " - A state $s_i$ is aperiodic if the \"period\" of the state is 1. The period $d(s_i) := gcd\\{n\\ge1: (P^n)_{i,i}>0\\}$, which says after d (or any interger times of d) steps, it is possible (positive probability) to return to the same state $i$.  \n",
        "  - A Markov chain is aperiodic if all the states are aperiodic.\n",
        "\n",
        "- *Stationary distribution $\\pi$*: \n",
        "  - Def1. $\\pi P = \\pi$, where $P$ is the transition matrix of the Markov chain.\n",
        "  - Def2. The distribution $\\mu_n$ (at step n in a Markov chain) converges in total variance (t.v.) to $\\pi$, i.e., $\\lim_{n\\to\\infty}d_{TV}(\\mu_n, \\pi) = 0$\n",
        "- *Reversible distribution*: \n",
        "A probability distribution $\\pi$ is said to be reversible on the Markov chain (or the transition matrix) if for any $i,j\\in \\{1,2,...,k\\}$, $\\pi_i P_{i,j} = \\pi_j P_{j,i}$. A Markov chain is reversible if it has at least one reversible distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvkIFbEx9iix",
        "colab_type": "text"
      },
      "source": [
        "# 1. Stationary Distribution of a Markov Chain\n",
        "\n",
        "## Why do we need \"aperiodic\"ï¼Ÿ\n",
        "Stationary distribution is an important property in Markov chain theory. For a discrete time and finite state space Markov chain, being \"irreducible\" and \"aperiodic\" implies it has one and only one stationary distribution. \n",
        "\n",
        "Compared with irreducibility, aperiodicity is a less intuitive concept. \n",
        "\n",
        "For state $s_i$, the definition of aperiodicty tells us that $gcd(N_i) := gcd\\{n\\ge1: (P^n)_{i,i}>0\\}=1$, but it does not necessarily mean that 1 is an element in $N_i$.In other words, we don't have $P_{i,i}>0$ yet.\n",
        "\n",
        "However, being aperiodic leads us to Thm 4.1, which says \"there exists a $N<\\inf$ such that $(P^n)_{i,i}>0$ for all $n>N$\". In other words, when $n>N$ and the chain move forward one step, we can always go from state $i$ to $i$ with positive probability. This is very nice as it **gives hopes for convergence when $n$ goes to infinity**. The proof of Thm 4.1 utilizes a lemma from number theory which guarantees that all the $n$ after $N$ belongs to the set $N_i$. \n",
        "\n",
        "## Example: Aperiodic or periodic Markov chain?\n",
        "To determine whether a Markov chain is aperiodic or not, there are some creterions that can be applied. \n",
        "\n",
        "Step 1. We can draw a transition diagram from the transition matrix if the number of states are not too many. With the diagram, we can easily tell if the Markov chain is irreducible or not. \n",
        "\n",
        "Step 2. It can be shown that **if a Markov chain is irreducible, all of its states have the same period**. \n",
        "![Example 1. Transition diagrams of two Markov chains](https://miro.medium.com/max/2404/1*bDNsx76wQoE9uyWJby1zwQ@2x.png)\n",
        "Take a look at the two MCs in the figure above. From the transition diagram, both chains are irreducible (as the states can all be reached from each other). \n",
        "Therefore, we can look at only one of their states for the full information of periodicity. \n",
        "\n",
        "For the chain on the left, state 1 has periods of 2, 4, ..., therefore its period is 2. State 1 is periodic and then the whole chain is not aperiodic. Simimlarly, look at state 1 (in the center). It has periods of 3, 6, ..., and thus its period is 3. The MC on the right is not aperiodic either. \n",
        "\n",
        "A useful tip:**If a Markov chain is irreducible and it has a state $i$ such that $P_{i,i}>0$, then it is aperiodic.** (adapted from Prb 4.2. in the book)\n",
        "\n",
        "*Proof*: $P_{i,i} > 0$ means the chain can come to state $i$ from state i in one step, and thus the period of i (gcd{$n\\ge1: P^n_{i,i}>0$}) is 1. As we mentioned in step2, all the states have the same period for irreducible MC, and thus the chain is aperiodic. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDk1ntROh3D1",
        "colab_type": "text"
      },
      "source": [
        "# 2. Reversible and Stationary\n",
        "We have seen the definition of \"stationary\" distribution and \"reversible\" distribution in the terminology part, but it is not that obvious why a reversible distribution is also a stationary one for the Markov chain. Let's prove it here. \n",
        "\n",
        "A distribution $\\pi$ is stationary means the distribution remains the same after the transition. Mathematically, it is $\\pi P=\\pi$. We can also write it in element-wise format, $\\sum_i \\pi_{i} P_{i,j} = \\pi_j$. NOTE: The distribution on the right hand side of the equation would not be $\\pi$ (but a different distribution) if it is not stationary. \n",
        "\n",
        "If $\\pi$ is reversible, we have $$\\sum_i \\pi_i P_{i,j} =\\sum_j \\pi_j P_{j,i},$$ so $$\\sum_i \\pi_i P_{i,j} = \\sum_i \\pi_j P_{j,i} = \\pi_j \\sum_i P_{j,i} = \\pi_j$$ and therefore, $\\pi$ is also a stationary distribution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVlAPLqmhdoo",
        "colab_type": "text"
      },
      "source": [
        "# 3. MCMC\n",
        "This week, we will move one step forward, from theory to algorithms and see how a popular algorithm called Markov Chain Monte Carlo (MCMC) could help us solve various problems. \n",
        "\n",
        "## 3.1. How to construct a MCMC in general: a \"secret\" receipy.  \n",
        "- Example: Gibbs sampler for \"hard-core\" model.\n",
        "\n",
        "- Example: Metropolis chain for ... (from my notes)\n",
        "\n",
        "The cool thing about MCMC is that it allows us to **construct a Markov chain with the desired $\\pi$ as a stationary distribution** (instead of simulating $\\pi$ directly). Moreover, **we have the freedom to define our own transition matrix $P$ (from $Q$) and achieve it through a \"propose-accept\" two step procedure** where the stochastic matrix $Q$ can be relatively easy to compute and simulate. \n",
        "\n",
        "In layman's words, a MCMC decompose the random \"jump\" into two steps (\"propose\" and \"accept\") and thus make it a lot easier to simulate.\n",
        "\n",
        "## 3.2. What is MCMC good for? Sampling a distribution; approximate counting; etc. \n",
        "When we think of a Markov chain, a vivid image in our minds is the \"jump\" from one state to another. The jump is \"random\", following a probability distribution ($P_{i,j}$ for the jump from state $i$ to $j$), and simulating this random jump for an arbitrary distribution is not trivial especially when the state space is large. \n",
        "\n",
        "(1). A \"naive\" method: \n",
        "\n",
        "Given a Markov chain with state space $S=\\{s_1, s_2, ..., s_k\\}$, we assume at step $n$, the state is $s_i$ ($X_n=s_i$). Then $X_{n+1}$ is determined from $s_i$ and a generated random number $x (x\\in[0,1])$ in following way: \n",
        "\n",
        "$\\phi(s_i, x) = \\left\\{\n",
        "                \\begin{array}{ll}\n",
        "                  s_1, x\\in[0, P_{i,1}) \\\\\n",
        "                  s_2, x\\in[P_{i,1}, P_{i,1}+P_{i,2})\\\\\n",
        "                  ...\\\\\n",
        "                  s_k, x\\in[\\sum_{j=1}^{k-1} P_{i,j}, 1]\n",
        "                \\end{array}\n",
        "              \\right.\n",
        " $\n",
        "\n",
        "\n",
        "There are two steps: \n",
        "- First generate a random number $x$; \n",
        "- second, find the next state corresponding to $x$. \n",
        "\n",
        "Note that if $P_{i,j}$ is arbitrary for different $j$, then we have to determine the next state ($j\\in[1,2,...,k]$) by finding where $x$ should be inserted into the cummulative probability (essentially a sorted list). Although we can use binary search to speed up the algorithm, it is still expensive if the state space is large.  \n",
        "\n",
        "To illusate this method, we code up the process to simulate a Markov chain for Los Angeles' weather. \n",
        "\n",
        "NOTE: Why is MCMC a better sampling method (in terms of computer simulation)? Code example: What we do when we don't have MCMC?\n",
        "\n",
        "NOTE: How can we estimate the distribution or even counts elements in a set without sampling all the possible states?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCK4N1kHkR1H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf9ff759-e9bd-449c-bb41-aea383a748f5"
      },
      "source": [
        "# Example. Los Angeles' weather\n",
        "# If we assume there are only two types of weather: sunny and not-sunny, we can model\n",
        "# the weather at different time as a Markov chain (state s1 is 'sunny' and s2 \n",
        "# is 'not sunny').\n",
        "\n",
        "# In Los Angeles, as sunshine is much more common than other weathers, the\n",
        "# transition matrix is assumed to be the following\n",
        "# (from 'not-sunny' to 'sunny' is P12=0.5, from 'sunny' to 'not-sunny' is P21=0.1): \n",
        "P = [[0.5, 0.5], \n",
        "     [0.1, 0.9]]\n",
        "\n",
        "# An initial distribution: \n",
        "u0 = [0.5, 0.5]\n",
        "\n",
        "N = 10 # The number of steps to simulate\n",
        "\n",
        "from bisect import bisect\n",
        "from random import random\n",
        "\n",
        "def init_chain_state(u0):\n",
        "  \"\"\"\n",
        "  u0: a list. \n",
        "    The distribution of initial state.\n",
        "\n",
        "  NOTE: Simulate the random state the same way as 'move_one_step()'; bisect \n",
        "  is used to find the index efficientlyã€‚\n",
        "  \"\"\"\n",
        "  target = [sum(u0[:i+1]) for i, ele in enumerate(u0)]\n",
        "  x = random()\n",
        "  state_index = bisect(target, x)\n",
        "  # print(x, target, state_index)\n",
        "\n",
        "  return state_index\n",
        "\n",
        "# For testing purpose: \n",
        "# u0 = [0.2, 0.5, 0.3]\n",
        "# init_chain_state(u0)\n",
        "\n",
        "def move_one_step(P, s_t, x):\n",
        "  \"\"\"\n",
        "  In a Markov chain process, move from step t to t+1 according to the transition\n",
        "  matrix. \n",
        "\n",
        "  P: a list of list. \n",
        "    The transition matrix. \n",
        "  s_t: integer.\n",
        "    The state at step t. \n",
        "    1 for 'not-sunny', 2 for 'sunny'.\n",
        "  x: float, between 0 and 1.\n",
        "\n",
        "  # The code can be simplified, but we write out the \"raw\" idea here to\n",
        "  # make it easy to compare to the algorithm description side by side. \n",
        "  \"\"\"\n",
        "  if s_t == 1:\n",
        "    # The if-statement below is the \"naive\" algorithm to select s_{t+1}:\n",
        "    if x < P[0][0]:\n",
        "      return 1\n",
        "    elif x < P[0][0] + P[0][1]:\n",
        "      return 2\n",
        "  \n",
        "  if s_t == 2:\n",
        "    if x < P[1][0]:\n",
        "      return 1\n",
        "    elif x < P[1][0] + P[1][1]:\n",
        "      return 2\n",
        "\n",
        "\n",
        "def simulate_N_steps(P, u0, N):\n",
        "  # Initial state:\n",
        "  s_t = 1 if init_chain_state(u0)==0 else 2\n",
        "\n",
        "  s_seq = []\n",
        "  for i in range(N):\n",
        "    x = random()\n",
        "    s_t = move_one_step(P, s_t, x)\n",
        "    s_seq.append(s_t)\n",
        "\n",
        "  return s_seq\n",
        "\n",
        "N = 1000\n",
        "s_seq = simulate_N_steps(P, u0, N)\n",
        "print(s_seq[-10:])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw6vzNqewlpL",
        "colab_type": "text"
      },
      "source": [
        "**On my (Markov china class) notes**:\n",
        "\n",
        "a. Section 12 to 14\n",
        "\n",
        "Multiple examples are introduced in this part. Including hard core problem, q-coloring problem, simulated annealing (for global optimization), Ising model, etc. A metropolis algorithm is provided for each of these problems. \n",
        "\n",
        "There are two gaps that need to be connected: \n",
        "One is the connection between the idea expressed in h/Q/P and those specific algorithmic solutions; the other one is to show what Q and h for each example (not obvious at all).\n",
        "\n",
        "b. Section 15\n",
        "\n",
        "Prop-Wilson algorithm is not easy to follow. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq57ZJKizUMZ",
        "colab_type": "text"
      },
      "source": [
        "## **Sampling Methods in General**:\n",
        "\n",
        "### 1. Motivation for sampling methods;\n",
        "Applications: \n",
        "- Estimate expectations and other statistics; \n",
        "- Visualize a random distribution (as a histogram); \n",
        "- Calculate area or integration (as a fraction of the bounding rectangular \"box\"); \n",
        "- Compute the normalizing constant to convert any functions to a probability distribution. \n",
        "\n",
        "Advantages: \n",
        "- Relatively easy to implement;\n",
        "- Wide applications (an \"all-purpose\" hammer). \n",
        "\n",
        "### 2. Monte Carlo method (or \"estimator\");\n",
        "The idea is simple, \"to use sample mean as an approximation to the expected value\" (and we need to generate samples according to the distribution first). \n",
        "\n",
        "### 3. Importantce sampling: \n",
        "Strictly speaking, this is not a \"sampling\" method as it does not generate samples directly; instead, it is able to estimate expectation of a function over any distribution. \n",
        "\n",
        "### 4. MCMC\n",
        "#### a. Metropolis Algorithm\n",
        "Different descriptions about Metropolis algorithm (in the class notes, in mathmonk's video, and in the book)?\n",
        "\n",
        "Why is Metropolis algorithm a MCMC method? What is the MC? Why does it satisfy the ergodic theorem? \n",
        "\n",
        "(If we use the version of M-algorithm in Mathmonk's video) The transition matrix is\n",
        "$P(x_i, x_j)=P(X_{n+1}=x_j|X_{n}=x_i)=P(x_j|x_i)P(\\textrm{accept}\\ x_j | (x_i, x_j))$\n",
        "where the first term is modeled by $Q_{i,j}$, and the second term is the probability $x_j$ is accepted. From here, we can further show that the M.C. is irreducible and aperiodic and thus we can apply the ergodic theorem. \n",
        "\n",
        "From the class notes: If we pick the Q(stochastic/proposing matrix) and h in a certain way, we always get a M.C. with specified stationary distribution. So we only need to show that we have a proper Q and h. \n",
        "\n",
        "\n",
        "#### b. Other examples of MCMC algorithms\n",
        "Gibbs sampler is a special case of Metropolis algorithm. It is good for the state space of the form of $S^V$, where V is the set of positions, and S is the attainable values at each $v\\in V$. Gibbs sampler is often used to sample a complex distribution. For example, Ising model. \n",
        "\n",
        "**_References_**:\n",
        "\n",
        "[1] Book section \"Generating Samples from Probability Distributions\": https://web.mit.edu/urban_or_book/www/book/chapter7/7.1.3.html \n",
        "\n",
        "[2] Notes \"Intro to Sampling Methods\" (from PSU course \"Math Tools for Computer Visions\"): http://www.cse.psu.edu/~rtc12/CSE586/lectures/cse586samplingPreMCMC.pdf\n",
        "\n",
        "[3] Videos about sampling methods (Machine Learning section 16 & 17), by \"mathematicalmonk\": https://www.youtube.com/watch?v=3Mw6ivkDVZc&list=PLD0F06AA0D2E8FFBA&index=132"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnbxrQKGyE_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "1b881d90-0267-4fcf-fb3f-de75b0addc73"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iajU1JtjbaEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JNXsUbgwlOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}